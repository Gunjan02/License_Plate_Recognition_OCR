# -*- coding: utf-8 -*-
"""OCR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_7gUM3kk7-VVTquvmIWYUApJ1Zd6SfQI
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/

# Commented out IPython magic to ensure Python compatibility.
# %cd My\ Drive

# Commented out IPython magic to ensure Python compatibility.
# %cd license_plate

"""I downloaded the dataset for License Plate Recognition from a github link for the cars found in Brazil. Uploaded the data to Google Drive under the name license_plate which has further folders of Images and Annotations. Images has images of the cars found in Brazil and annotations has the annotations for the license plates.

---


Github link for dataset - https://github.com/openalpr/benchmarks/tree/master/endtoend/br

---


Total Images - 114
"""

import cv2
import numpy as np
from matplotlib import pyplot as plt
from google.colab.patches import cv2_imshow
import glob
import os
import imutils

"""Extracting all the images and storing in a list to further do the Image Preprocessing using OpenCV"""

img_dir = "Preprocessed_Images/" # Enter Directory of all images 
img_data_path = os.path.join(img_dir,'*g')
img_files = glob.glob(img_data_path)
img_data = []
for f in img_files:
    img_data.append(f)

len(img_data)

"""Extracting all the .txt files for annotations and storing in a list to further get the boundary boxes."""

ann_dir = "Annotations/" # Enter Directory of all images 
ann_data_path = os.path.join(ann_dir,'*.txt')
ann_files = glob.glob(ann_data_path)
ann_data = []
for f in ann_files:
  ann_data.append(f)

len(ann_data)

for img in img_files:
  image = cv2.imread(img)
  cv2_imshow(image)

"""Converting the RGB Image to Grayscale, Filtering the image for better processing and draw boundary boxes around the license plate, using the annotations.

---


Region Segmentation
"""

for img in img_files:
  image = cv2.imread(img)
  gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) # grayscale
  _,thresh = cv2.threshold(gray,150,255,cv2.THRESH_BINARY_INV) 
  kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))
  dilated = cv2.dilate(thresh,kernel,iterations = 13) # dilate
  contours, hierarchy = cv2.findContours(dilated,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE) 
# # get contours
# # for each contour found, draw a rectangle around it on original 
# image
  for ann in ann_files:
    if (img.split('/')[-1].split('.jpg')[0]) == (ann.split('/')[-1].split('.txt')[0]):
      print(img)
      print(ann)
      lines = []
      with open(ann) as file:
        for line in file:
          line = line.split('\t')
          lines.append(line)
          for contour in contours:
            [x,y,w,h] = [int(lines[0][1]),	int(lines[0][2]),	int(lines[0][3]),	int(lines[0][4])]
    # discard areas that are too large
 
            cv2.rectangle(gray,(x,y),(x+w,y+h),(255,0,255),2)
# write original image with added contours to disk  
 
  cv2_imshow(gray)

"""Cropping the Region of Interest , here being the license plate surrounded by Boundary Box for further classification and Prediction. Once cropped, storing in Google Drive."""

for img in img_files:
  image = cv2.imread(img)
  gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) # grayscale
  _,thresh = cv2.threshold(gray,150,255,cv2.THRESH_BINARY_INV) 
  kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))
  dilated = cv2.dilate(thresh,kernel,iterations = 13) # dilate
  contours, hierarchy = cv2.findContours(dilated,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE) 
# # get contours
# # for each contour found, draw a rectangle around it on original 
# image
  for ann in ann_files:
    if (img.split('/')[-1].split('.jpg')[0]) == (ann.split('/')[-1].split('.txt')[0]):
      print(img)
      print(ann)
      lines = []
      with open(ann) as file:
        for line in file:
          line = line.split('\t')
          lines.append(line)
          for contour in contours:
            [x,y,w,h] = [int(lines[0][1]),	int(lines[0][2]),	int(lines[0][3]),	int(lines[0][4])]
    # discard areas that are too large
 
            cv2.rectangle(gray,(x,y),(x+w,y+h),(255,0,255),2)
# write original image with added contours to disk  
            roi = gray[y:y+h,x:x+w]
 
  cv2_imshow(roi)

import os
import fnmatch
import cv2
import numpy as np
import string
import time

from keras.preprocessing.sequence import pad_sequences

from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional,GRU,add,concatenate,Activation
from keras.models import Model
from keras.activations import relu, sigmoid, softmax
import keras.backend as K
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint
import tensorflow as tf

"""To preprocess the output labels I read the text from the name of the image as the image name contains text written inside the image.

---


Encode each character of a word into some numerical value by creating a function( as ‘a’:0, ‘b’:1 …….. ‘z’:25 etc ). Let say we are having the word ‘abab’ then our encoded label would be [0,1,0,1]

---


Compute the maximum length from words and pad every output label to make it of the same size as the maximum length. This is done to make it compatible with the output shape of the RNN architecture.
"""

# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
# total number of our output classes: len(char_list)
char_list = string.ascii_uppercase+string.digits
print(char_list)
def encode_to_labels(txt):
    # encoding each output word into digits
    dig_lst = []
    for index, char in enumerate(txt):
        try:
            dig_lst.append(char_list.index(char))
        except:
            print(char)
        
    return dig_lst

path = 'New_Processed_Images/'
# lists for training dataset
training_img = []
training_txt = []
train_input_length = []
train_label_length = []
orig_txt = []
 
#lists for validation dataset
valid_img = []
valid_txt = []
valid_input_length = []
valid_label_length = []
valid_orig_txt = []
 
max_label_len = 0
 
i =1 
flag = 0

img_dir = "New_Processed_Images/" # Enter Directory of all images 
img_data_path = os.path.join(img_dir,'*g')
img_files = glob.glob(img_data_path)
img_data = []
for f in img_files:
    img_data.append(f)

plates_dir = "plates/" # Enter Directory of all images 
plates_data_path = os.path.join(plates_dir,'*g')
plates_files = glob.glob(plates_data_path)
plates_data = []
for f in plates_files:
    plates_data.append(f)

plates_files

"""Resized the image to fit the input shape required for the CRNN Architecture. Already done, so Only run one time."""

# for img in img_files:
#   i=cv2.imread(img)
#   resized = cv2.resize(i,(128,32))
#   print(resized.shape)
#   cv2_imshow(resized)
#   cv2.imwrite(img.split('/')[-1],resized)

# img = cv2.imread(plates_files[0])
# img.reshape(32,128,1)

"""To make it acceptable for the model we need to use some preprocessing. We need to preprocess both the input image and output labels. 

---


To preprocess input image-
1.Read the cropped gray scaled image
2.Made each image of size (128,32) by using padding
Expand image dimension as (128,32,1) to make it compatible with the input shape of architecture
3.Normalized the image pixel values by dividing it with 255.
"""

j=1
for filenames in plates_files:
  print(filenames)
  img = cv2.imread(filenames)  
  w, h, z = img.shape
  print("w ::",w)
  print("h ::",h)
  print("z ::",z)

  # Normalize each image
  img = img/255
        
  # get the text from the image
  txt = filenames.split('/')[-1].split('.jpg')[0]
  print(txt)      
  # compute maximum length of the text
  if len(txt) > max_label_len:
    max_label_len = len(txt)
  
  

  if j%10 == 0:
    print("J",j)
    valid_orig_txt.append(txt)   
    valid_label_length.append(len(txt))
    valid_input_length.append(31)
    valid_img.append(img)
    valid_txt.append(encode_to_labels(txt))
  else:
    print("J",j)
    orig_txt.append(txt)   
    train_label_length.append(len(txt))
    train_input_length.append(31)
    training_img.append(img)
    training_txt.append(encode_to_labels(txt))
  j=j+1

#7 being the max number of characters required for license plate recognition
max_label_len

#this has the validation data
valid_orig_txt

valid_label_length

#validation images, having normalized pixel values
valid_img

#Encoded validation data for license plate characters. A:0,B:1,C:2 and so on..
valid_txt

training_txt

# pad each output label to maximum text length
 
train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))
valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))

"""Model = CNN + RNN + CTC loss
The convolutional neural network to extract features from the image
Recurrent neural network to predict sequential output per time-step
CTC loss function which is transcription layer used to predict output for each time step.


---


Input shape for architecture having an input image of height 32 and width 128.
Used seven convolution layers of which 6 are having kernel size (3,3) and the last one is of size (2.2). And the number of filters is increased from 64 to 512 layer by layer.

---


Two max-pooling layers are added with size (2,2) and then two max-pooling layers of size (2,1) are added to extract features with a larger width to predict long texts.

---


Used batch normalization layers after fifth and sixth convolution layers which accelerates the training process.

---


Then used a lambda function to squeeze the output from conv layer and make it compatible with LSTM layer.

---


Then used two Bidirectional LSTM layers each of which has 128 units. This RNN layer gives the output of size (batch_size, 31, 37). Where 37(26 for Alphabets, 10 for digits and 1 for blank character) is the total number of output classes including blank character.
"""

# input with shape of height=32 and width=128 
inputs = Input(shape=(32,128,3))
 
# convolution layer with kernel size (3,3)
conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)
# poolig layer with kernel size (2,2)
pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)
 
conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)
pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)
 
conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)
 
conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)
# poolig layer with kernel size (2,1)
pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)
 
conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)
# Batch normalization layer
batch_norm_5 = BatchNormalization()(conv_5)
 
conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)
batch_norm_6 = BatchNormalization()(conv_6)
pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)
 
conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)
 
squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)
 
# bidirectional LSTM layers with units=128
blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)
blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)
 
outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)

# model to be used at test time
act_model = Model(inputs, outputs)

act_model.summary()

"""CTC loss function helps us to prevent annotating each time step and help us to get rid of the problem where a single character can span multiple time step which needs further processing if we do not use CTC.


---
A CTC loss function requires four arguments to compute the loss, predicted outputs, ground truth labels, input sequence length to LSTM and ground truth label length. To get this we need to create a custom loss function and then pass it to the model.
"""

labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')
input_length = Input(name='input_length', shape=[1], dtype='int64')
label_length = Input(name='label_length', shape=[1], dtype='int64')
 
 
def ctc_lambda_func(args):
    y_pred, labels, input_length, label_length = args
 
    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)
 
 
loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])

#model to be used at training time
model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)

model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')
 
filepath="best_model.hdf5"
checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
callbacks_list = [checkpoint]

training_img = np.array(training_img)
train_input_length = np.array(train_input_length)
train_label_length = np.array(train_label_length)

valid_img = np.array(valid_img)
valid_input_length = np.array(valid_input_length)
valid_label_length = np.array(valid_label_length)

batch_size = 128
epochs = 30
model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)

"""I do get the prediction for validation data in numpy array form, and it gets decoded for some validation data recognizing only the first character for the final character prediction."""

act_model.load_weights('best_model.hdf5')
 
# predict outputs on validation images
prediction = act_model.predict(valid_img[:10])
print(prediction)
# use CTC decoder
out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],
                         greedy=False)[0][0])
 
# see the results
i = 0
for x in out:
    print("original_text =  ", valid_orig_txt[i])
    print("predicted text = ", end = '')
    for p in x:  
        if int(p) != -1:
            print(char_list[int(p)], end = '')       
    print('\n')
    i+=1

